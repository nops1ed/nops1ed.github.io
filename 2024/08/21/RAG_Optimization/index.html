<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="nops1ed">





<title>RAG Optimization | nops1ed&#39;s Blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">nops1ed&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">nops1ed&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">RAG Optimization</h1>
            
                <div class="post-meta">
                    

                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="RAG-updating"><a href="#RAG-updating" class="headerlink" title="RAG (updating)"></a>RAG (updating)</h1><p>Writing a blog post to record experiences and optimization ideas for RAG (Retrieve-and-Generate architecture) that is currently being used in the team, which is an early preview. Let’s start by discussing why we chose to use RAG.</p>
<h2 id="Why-RAG"><a href="#Why-RAG" class="headerlink" title="Why RAG ?"></a>Why RAG ?</h2><p>After the popularity of large models, some companies found that their existing large models could not answer some company-specific content. As a result, they created knowledge bases tailored to large models, using internal corporate background knowledge to enhance the ability of large models to provide personalized answers. </p>
<p>However, this raises issues of information leakage. As a result, most companies opt for private deployment models to solve this problem, which is what we call enterprise knowledge graphs with private deployment.</p>
<p>If without RAG, LLM is only knowledge source  </p>
<table>
<thead>
<tr>
<th>Without RAG</th>
<th>With RAG</th>
</tr>
</thead>
<tbody><tr>
<td>Hallucination</td>
<td>Source data reference</td>
</tr>
<tr>
<td>Outdated info</td>
<td>Latest info</td>
</tr>
<tr>
<td>Knowledge blind spot</td>
<td>Covers all search engine information</td>
</tr>
<tr>
<td>Doesn’t cover my data</td>
<td>Covers my &#x2F; public data</td>
</tr>
</tbody></table>
<h2 id="RAG-Paradigm"><a href="#RAG-Paradigm" class="headerlink" title="RAG Paradigm"></a>RAG Paradigm</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.10997">Retrieval-Augmented Generation for Large Language Models: A Survey (click me to view the paper)</a></p>
<h2 id="Using-Naive-RAG"><a href="#Using-Naive-RAG" class="headerlink" title="Using Naive RAG"></a>Using Naive RAG</h2><p>AKA Retrieve-then-read Architecture</p>
<p>The most classic RAG structure steps are as follows: Establish index –&gt; Retrieve –&gt; LLM Combining Generate Information to Answer Questions</p>
<ul>
<li>Establish Index<ul>
<li>Generally, this step is performed offline. The document data is cleaned and divided into chunks, and an embedding model is used to generate vectors to create an index (which is then stored in a database)</li>
</ul>
</li>
<li>Retrieve<ul>
<li>Once the KB is established, user could submit a query, we will use the previous embedding model to generate a vector and perform similarity matching with the KB. Select the top k similar results as the enhancement information for the current question.</li>
</ul>
</li>
<li>Combine LLM<ul>
<li>Based on the above information, generate a prompt and combine it with the LLM to answer the user’s query.</li>
</ul>
</li>
</ul>
<p>See more details are as follows:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Local Documents   ---&gt; Unstructed Loader    --&gt; Text ---&gt; Text Splitter ---&gt; Text Chunks</span><br><span class="line">                                                                                |</span><br><span class="line">                                                                                |</span><br><span class="line">                                                                                v</span><br><span class="line">                                                                            Embedding</span><br><span class="line">                                                                                |</span><br><span class="line">                                                                                |</span><br><span class="line">                                                                                v</span><br><span class="line">Query      ---&gt;     Embedding ---&gt;  Query Vector ---&gt;  Vector Similarity &lt;--- VectorStore</span><br><span class="line">                                                                |</span><br><span class="line">                                                                |</span><br><span class="line">                                                                v</span><br><span class="line">                        Prompt  &lt;---  Prompt Template  &lt;--- Related Text chunks</span><br><span class="line">                          |          </span><br><span class="line">                          |</span><br><span class="line">                          v</span><br><span class="line">            Answer &lt;---  LLM</span><br><span class="line"></span><br><span class="line">                                                                                        <span class="comment">// Naive RAG</span></span><br></pre></td></tr></table></figure>

<p>However, such a structure never quite meets expectations and is prone to “garbage in, garbage out.” What’s wrong?</p>
<ul>
<li>Unable to efficiently match with KB. Possible reasons:<ul>
<li>Inefficient queries: whether intentional or unintentional, users may submit queries containing a lot of irrelevant information or fail to highlight their core concerns, causing the topic to deviate.</li>
<li>When establishing the index, unable to retrieve core knowledge</li>
</ul>
</li>
<li>LLM’s well-known problems:<ul>
<li>Unable to retrieve relevant knowledge or low-quality knowledge leads to hallucinations - self-generated text that is meaningless in RAG.</li>
<li>LLM can only reproduce KB information and cannot add more insightful or comprehensive information.</li>
</ul>
</li>
</ul>
<h2 id="Build-Advanced-RAG"><a href="#Build-Advanced-RAG" class="headerlink" title="Build Advanced RAG"></a>Build Advanced RAG</h2><h3 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h3><h4 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h4><p>PostgreSQL + pgvetcor</p>
<p>One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are ‘most similar’ to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.</p>
<h4 id="Parser-Augmentation"><a href="#Parser-Augmentation" class="headerlink" title="Parser Augmentation"></a><strong>Parser Augmentation</strong></h4><p>The success of RAG implementation hinges on whether the document interpretation meets expectations. Only by thoroughly extracting and interpreting the information within documents can a high-performance knowledge base be built to support LLMs. The first step, and arguably the most crucial one, is to optimize the document parsing process.  </p>
<p>Let’s take a typical PDF file as an example and compare the strengths and weaknesses of several mainstream PDF parsing effects:  </p>
<ul>
<li><p><strong>Direct Text Extraction</strong>: PyMUPDF, PyPDF, PDFMiner, etc.</p>
<ul>
<li>As a common Python PDF loader, these tools have the advantage of being relatively mature, low latency, and suitable for fast text extraction from PDF or documents.</li>
<li>However, they are not effective in handling complex document structures, such as tables, images, and layouts, which may not meet our expectations for RAG document parsing effects.</li>
</ul>
</li>
<li><p><strong>OCR Conversion and Text Extraction</strong>: Tesseract, Textract, etc.</p>
<ul>
<li>These tools are relatively mature and can capture document structure and layout.</li>
<li>However, they have high latency, and the performance varies depending on specific use cases.</li>
</ul>
</li>
<li><p><strong>Intelligent Document Parsing</strong>: LlamaParse, Unstructured.io, AzureDoc Intelligence, etc.</p>
<ul>
<li>These tools can parse complex document structures and convert them into Markdown format (or JSON, etc.).</li>
<li>However, they are relatively immature, have limited scalability, and high latency.</li>
</ul>
</li>
</ul>
<h4 id="Query-Translation"><a href="#Query-Translation" class="headerlink" title="Query Translation"></a><strong>Query Translation</strong></h4><p>The problem is, the chunk of information where the answer lies, might not be similar to what the user is asking. The question can be badly written, or expressed differently to what we expect. And, if our RAG app can’t find the information needed to answer the question, it won’t answer correctly.   </p>
<p>In fact, semantic search iom embedding is hard to get right. Embedding long documents is a challenge. User queries are a challenge, if a user provides an ambiguous query they’ll get ambiguous matches.<br>LLMs just follow what in the context and hallucinate answers as a result.  </p>
<ul>
<li><p><strong>Decomposition</strong>: If a question contains multiple independent sub-questions, split it and execute each one independently.</p>
<ul>
<li>IR_CoT (Information Retrieval for Contextual Text)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.10625.pdf">Least-to-Most (click me to view the paper)</a>: process least relevant information first</li>
</ul>
</li>
<li><p><strong>Enhancing Queries</strong>: If the retrieval method is sensitive to query content, generate multiple versions of the query to retrieve more relevant content.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.10496">HyDE (click me to view the paper)</a>        <ul>
<li>HyDE circumvents the aforementioned learning problem by performing search in documentonly embedding space that captures documentdocument similarity.</li>
</ul>
</li>
<li>Step-back prompting<ul>
<li>Few shot prompt to produce more abstract step_back question</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>We can also use a Generative AI model to rewrite the question. This model could be a large model, like (or the same as) the one we use to answer the question in the final step. Or it can also be a smaller model, specially trained to perform this task.  </p>
<ul>
<li><p><strong>Rewriting Queries</strong>:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.07678">Query2doc: Query Expansion with Large Language Models (click me to view the paper)</a>: uses LLMs to expand queries</li>
<li>RAG-Fusion: combines multiple queries or documents to form a new query</li>
</ul>
</li>
<li><p><strong>Trainable Rewriter</strong>:</p>
<ul>
<li>We can fine-tune a pre-trained model to perform the query rewriting task. Instead of relying on examples, we can teach it how query rewriting should be done to achieve the best results in context retrieving. Also, we can further train it using Reinforcement Learning so it can learn to recognize problematic queries and avoid toxic and harmful phrases. Or we can also use an open-source model that has already been trained by somebody else on the task of query rewriting.</li>
</ul>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">More abstraction    ^           Step-back <span class="title function_">question</span> <span class="params">(Step-back prompting)</span></span><br><span class="line">                    |                      ^</span><br><span class="line">                    |                      |</span><br><span class="line">                    |                      |</span><br><span class="line">                    |                      |</span><br><span class="line">                    |                  Question    -----&gt;  Re-<span class="title function_">written</span> <span class="params">(RAG-Fusion, Multi-Query)</span></span><br><span class="line">                    |                      |</span><br><span class="line">                    |                      |</span><br><span class="line">                    |                      |</span><br><span class="line">                    |                      v</span><br><span class="line">Less abstraction    v              Sub-<span class="title function_">question</span> <span class="params">(Least-to-Most)</span></span><br></pre></td></tr></table></figure>

<h3 id="Indexing"><a href="#Indexing" class="headerlink" title="Indexing"></a>Indexing</h3><h4 id="RAPTOR-Recursive-Abstractive-Processing-for-Tree-Organized-Retrieval"><a href="#RAPTOR-Recursive-Abstractive-Processing-for-Tree-Organized-Retrieval" class="headerlink" title="RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)"></a>RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.18059">Stanford, 31 Jan 2024 (click me to view the paper)</a></p>
<blockquote>
<p>We show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy</p>
</blockquote>
</blockquote>
<h4 id="Multi-representation-Indexing"><a href="#Multi-representation-Indexing" class="headerlink" title="Multi-representation Indexing"></a>Multi-representation Indexing</h4>
        </div>

        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/RAG/"># RAG</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2024/08/21/RAG%20Optimization/">RAG Optimization</a>
            
            
            <a class="next" rel="next" href="/2023/11/01/kaggle%E5%86%85%E5%8D%B7%E5%90%88%E9%9B%86/">Kaggle Code</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="slogan">
        <span>Home,sweet home<a href="" target="_blank"></a></span>
    </div>
</footer>

    </div>
</body>

</html>